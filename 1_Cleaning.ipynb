{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa7aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 06:39:16,781 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import warnings\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "!sed -i 's/hadoop.root.logger=INFO,console/hadoop.root.logger=WARN,console/' /usr/hadoop-3.3.2/etc/hadoop/log4j.properties\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "conf = pyspark.SparkConf().setAll(\n",
    "    [('spark.master', 'local[2]'),\n",
    "     ('spark.app.name', 'Cleaning')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cd18a4",
   "metadata": {},
   "source": [
    "### 1. Load Dataset\n",
    "\n",
    "The dataset was procured as a csv from the [City of Chicago](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present-Dashboard/5cd6-ry5g). To keep even distribution of data across seasons, 2023 data will be removed immediately (incomplete year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235982ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 06:39:46,416 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 7711864, Columns: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# csv was renamed to `crime.csv`\n",
    "path = 'home/work/Project/mas-dse-230/crime.csv'\n",
    "\n",
    "df = spark.read.csv(f'file:///{path}', header=True, inferSchema=True)\n",
    "df.createOrReplaceTempView('df')\n",
    "df = df.filter(\"Year < 2023\")\n",
    "\n",
    "print(f'Rows: {df.count()}, Columns: {len(df.schema)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73b02d8",
   "metadata": {},
   "source": [
    "### 2. Null Handling\n",
    "\n",
    "It's possible that the distribution of null is time-dependent due to changing data practices. As 7.7M records is already plenty of data, imbalanced years can likely be dropped with minimal impact so long as it comes from an extreme of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d0187d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---------------------+------------------+\n",
      "|Year|total_records|records_without_nulls|proportion_removed|\n",
      "+----+-------------+---------------------+------------------+\n",
      "|2022|       238418|               231291|            0.0299|\n",
      "|2021|       208616|               201259|            0.0353|\n",
      "|2020|       212115|               206405|            0.0269|\n",
      "|2019|       261260|               256918|            0.0166|\n",
      "|2018|       268786|               261666|            0.0265|\n",
      "|2017|       269082|               263038|            0.0225|\n",
      "|2016|       269793|               265316|            0.0166|\n",
      "|2015|       264759|               256729|            0.0303|\n",
      "|2014|       275735|               272528|            0.0116|\n",
      "|2013|       307470|               305056|            0.0079|\n",
      "|2012|       336264|               333751|            0.0075|\n",
      "|2011|       351965|               349473|            0.0071|\n",
      "|2010|       370496|               368307|            0.0059|\n",
      "|2009|       392823|               384159|            0.0221|\n",
      "|2008|       427167|               418048|            0.0213|\n",
      "|2007|       437084|               433874|            0.0073|\n",
      "|2006|       448175|               443611|            0.0102|\n",
      "|2005|       453771|               448002|            0.0127|\n",
      "|2004|       469421|               465257|            0.0089|\n",
      "|2003|       475980|               469978|            0.0126|\n",
      "|2002|       486804|               343581|            0.2942|\n",
      "|2001|       485880|                 3885|             0.992|\n",
      "+----+-------------+---------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "denulled = df.dropna().groupBy('Year').count()\n",
    "denulled.createOrReplaceTempView('denulled')\n",
    "\n",
    "query = '''\n",
    "    WITH cte AS (\n",
    "        SELECT Year, COUNT(*) AS total_records\n",
    "        FROM df\n",
    "        GROUP BY Year\n",
    "    )\n",
    "    SELECT cte.Year, cte.total_records, denulled.count AS records_without_nulls\n",
    "        , ROUND(1 - denulled.count/cte.total_records, 4) AS proportion_removed\n",
    "    FROM cte\n",
    "    INNER JOIN denulled\n",
    "        ON denulled.Year = cte.Year\n",
    "    ORDER BY Year DESC\n",
    "'''\n",
    "spark.sql(query).show(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9098e7",
   "metadata": {},
   "source": [
    "Nearly all of 2001 had records with nulls. 2002 had a significant proportion of null-containing records as well (29%).\n",
    "\n",
    "These years will be removed as will all null records since they account for a relatively small proportion of the total dataset. This null removall may impact the accuracy of downstream analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f5d5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6634666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(\"Year > 2002\").dropna()\n",
    "df.createOrReplaceTempView('df')\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f192b7e",
   "metadata": {},
   "source": [
    "### 3. Category Consolidation\n",
    "\n",
    "The features `Primary Type` and `Description` (to be further explored in the next notebook) contain categorical values that describe a crime. Many of these labels appear to be overlapping or text variations of each other, for example, `NON-CRIMINAL` and `NON - CRIMINAL`.\n",
    "\n",
    "The next cells will consolidate these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78cf4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptype_consolidation = {\n",
    "    'CRIM SEXUAL ASSAULT': 'CRIMINAL SEXUAL ASSAULT',\n",
    "    'NON-CRIMINAL (SUBJECT SPECIFIED)': 'NON-CRIMINAL',\n",
    "    'NON - CRIMINAL': 'NON-CRIMINAL',\n",
    "    'OTHER NARCOTIC VIOLATION': 'NARCOTICS',\n",
    "    'PUBLIC INDECENCY': 'PUBLIC INDECENCY/OBSCENITY',\n",
    "    'OBSCENITY': 'PUBLIC INDECENCY/OBSCENITY'\n",
    "}\n",
    "desc_consolidation = {\n",
    "    'AGGRAVATED':'AGG',\n",
    "    'ATTEMPT':'ATT',\n",
    "    'CRIMINAL':'CRIM',\n",
    "    'POSSESSION':'POSS',\n",
    "    'POS ':'POSS',\n",
    "    'POSESS:':'POSS ',\n",
    "    'POSESS ':'POSS ',    \n",
    "    'REGISTRATION':'REG',\n",
    "    'PRO. ':'PROFESSIONAL ',\n",
    "    'PO ': 'POLICE OFFICER ',\n",
    "    'P.O.':'POLICE OFFICER',\n",
    "    'RITUAL': 'RIT',\n",
    "    'MUTILATION':'MUT',\n",
    "    'INSTRUMENT':'INSTR',\n",
    "    'MANU/DEL:':'MANUFACTURE DELIVER',\n",
    "    'MANU/DELIVER:':'MANUFACTURE DELIVER',\n",
    "    'MANUFACTURE / DELIVER':'MANUFACTURE DELIVER'\n",
    "}\n",
    "\n",
    "for k, v in ptype_consolidation.items():\n",
    "    df = df.replace(k, v, 'Primary Type')\n",
    "    \n",
    "replacement_expr = col('Description')\n",
    "for k, v in desc_consolidation.items():\n",
    "    replacement_expr = regexp_replace(replacement_expr, k, v)\n",
    "df = df.withColumn('Description', replacement_expr)\n",
    "df = df.withColumn('Description', regexp_replace(df['Description'], '[^A-Za-z0-9]', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513be690",
   "metadata": {},
   "source": [
    "Cleaning is now done, specify the output path to generate the new csv which will be the input for the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c72d60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "write_path = 'home/work/Project/mas-dse-230/cleaned_crime.csv'\n",
    "df.coalesce(1).write.mode('overwrite').csv(f'file:///{write_path}', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff57619",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe921c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
